{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJavadHzr/LLMs/blob/master/ETH_LLM_Assignment2_Q4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXt6LU7yCthu"
      },
      "source": [
        "# Assignment 2 - RAG\n",
        "\n",
        "Parts which require your interaction are marked with `TODO:`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JwUEQnwCuh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168dd140-80dc-4199-e7a7-09d31df1fafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, sentence_transformers, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 sentence_transformers-3.2.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install the relevant dependencies\n",
        "!pip3 install datasets sentence_transformers tqdm numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNC0Um-19gtf"
      },
      "source": [
        "Imagine your task is to build a question-answering (QA) system for a company. You are given a language model and have to create this product out of it.\n",
        "The requirements of the system need to adapt very quickly to the new data without training.\n",
        "For this, we will use **Retrieval Augmented Generation (RAG)**.\n",
        "The company insists you use their in-house LM model trained on multiple tasks, a _flan-t5-small_.\n",
        "You can test its QA functionality by asking the question _\"When ETH was founded?\"_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N1qUaDZCthv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cf02f4f7-f083-4d87-9d6b-9914db63c2b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1897'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Example inference with the model.\n",
        "# TODO: run me to test the environment\n",
        "\n",
        "# TODO: if you are using Colab, make sure to go to Runtime->Change runtime type and select GPU\n",
        "# if you are running this without GPU (not recommended), remove `device=0`.\n",
        "from transformers import pipeline\n",
        "vanilla_qa_pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device=0, truncation=True)\n",
        "\n",
        "QUESTION = \"QUESTION: When was ETH founded?\"\n",
        "\n",
        "vanilla_qa_pipe(f\"{QUESTION} ANSWER:\", max_new_tokens=10)[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNIbWYrC9gtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "beda7264-7acb-48e1-d6f1-5a1270bc6b2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1854'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "vanilla_qa_pipe(f\"\"\"\n",
        "        CONTEXT: ETH Zurich (German: Eidgenoessische Technische Hochschule Zurich; English:\n",
        "        Federal Institute of Technology Zurich) is a public research university in Zurich,\n",
        "        Switzerland. Founded in 1854 with the stated mission to educate engineers and scientists,\n",
        "        the university focuses primarily on science, technology, engineering, and mathematics. It\n",
        "        consistently ranks among the top universities in the world and its 16 departments span a\n",
        "        variety of disciplines and subjects.\n",
        "        {QUESTION}\n",
        "        ANSWER:\",\n",
        "    \"\"\",\n",
        "    max_new_tokens=10\n",
        ")[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7E7ivT39gtf"
      },
      "source": [
        "The first output is 1897, which is incorrect.\n",
        "\n",
        "This is not a problem, we can use RAG to automatically provide the passage from an [external source](https://en.wikipedia.org/wiki/ETH_Zurich) and make the model answer. Concatenating the first paragraph from Wikipedia to the question makes the model yield the correct answer 1854."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvfbw-XVCthw"
      },
      "outputs": [],
      "source": [
        "# Define model function; do not modify\n",
        "from typing import List\n",
        "\n",
        "def rag_qa_pipe(question: str, passages: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Define the RAG pipeline which concatenates passages to the question.\n",
        "    :param question: Question text.\n",
        "    :param passages: Relevant text passages.\n",
        "    :return: Generated text from the pipeline.\n",
        "    \"\"\"\n",
        "    passages = \"\\n\".join([f\"CONTEXT: {c}\" for c in passages])\n",
        "    return vanilla_qa_pipe(f\"{passages}\\nQUESTION: {question}\\nANSWER: \", max_new_tokens=10)[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKLM2jX59gtf"
      },
      "source": [
        "To make sure you understand the function `rag_qa_pipe`, ask some question without and with some relevant context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJIxcohU9gtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2632259-9b9c-4539-b1c5-db7466584d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 September 1897\n",
            "14 March 1879\n"
          ]
        }
      ],
      "source": [
        "# TODO: use rag_qa_pipeline some random question that you might have just to test this function\n",
        "\n",
        "context=\"\"\"\n",
        "Albert Einstein; (14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is widely held as one of the most influential scientists. Best known for developing the theory of relativity, Einstein also made important contributions to quantum mechanics.[1][6] His mass–energy equivalence formula E = mc2, which arises from special relativity, has been called \"the world's most famous equation\".[7] He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\",[8] a pivotal step in the development of quantum theory.\n",
        "\"\"\"\n",
        "\n",
        "print(rag_qa_pipe(\"When was Einstein born?\", []))\n",
        "print(rag_qa_pipe(\"When was Einstein born?\", [context]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMkyWZa9gtg"
      },
      "source": [
        "Start with the provided model and the first 500 questions from the validation part of the _SQuAD_ dataset. The dataset has a ground truth Wikipedia passage linked to it and you can directly use it.\n",
        "\n",
        "Then, compute the QA performance of the model with and without prepended passage using `rag_qa_pipe(question, passages)`.\n",
        "\n",
        "Report the average case-sensitive answer exact match (model output is identical to the gold answer, EM) and case-insensitive [answer F1 scores](https://kierszbaumsamuel.medium.com/f1-score-in-nlp-span-based-qa-task-5b115a5e7d41) (F1) for both setups.\n",
        "Because each question has multiple possible answers, take the maximum score for a model answer across all gold answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boC3-WoQCthx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7d9298-74db-40f4-ff32-4c96e1820d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:04<00:00,  7.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "w passages:\tF1:0.18,\tEM: 0.31\n",
            "w/o passages:\tF1:0.03,\tEM: 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# baseline model evaluation\n",
        "# TODO: the this cell requires <30 new lines\n",
        "\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"rajpurkar/squad\")\n",
        "\n",
        "def metric_exact_match(ans_pred: str, ans_true: str) -> float:\n",
        "    \"\"\"\n",
        "    Case-sensitive answer exact match, model output is identical to the gold answer.\n",
        "    :param ans_pred: Predicted answer\n",
        "    :param ans_true: Ground truth answer\n",
        "    :return: 1. if the answers are the same, 0. otherwise\n",
        "    \"\"\"\n",
        "    # TODO: ~1 line\n",
        "    return 1. * (ans_pred == ans_true)\n",
        "\n",
        "def metric_f1(ans_pred: str, ans_true: str) -> float:\n",
        "    \"\"\"\n",
        "    Case-insensitive answer F1 score. Use white-space separated words as \"tokens\".\n",
        "    :param ans_pred: Predicted answer.\n",
        "    :param ans_true: Ground truth answer.\n",
        "    :return: F1 score between the predicted and ground truth answers.\n",
        "    \"\"\"\n",
        "    # TODO: ~10 lines\n",
        "    pred_tokens = ans_pred.split(' ')\n",
        "    true_tokens = ans_true#.split(' ')\n",
        "    TP = len([x for x in pred_tokens if x in true_tokens])\n",
        "    FP = len(pred_tokens) - TP\n",
        "    FN = len([x for x in true_tokens if x not in pred_tokens])\n",
        "    return TP / (TP + .5 * (FP + FN))\n",
        "\n",
        "\n",
        "w_f1, w_em = [], []\n",
        "wo_f1, wo_em = [], []\n",
        "for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "    # hint: use `line[\"question\"]`, `line[\"context\"]`, and `line[\"answers\"]`\n",
        "    # TODO: run with and without prepended passage\n",
        "    ans_true = ' '.join(set(line['answers']['text']))\n",
        "    w_pred = rag_qa_pipe(line['question'], passages=[line['context']])\n",
        "    wo_pred = rag_qa_pipe(line['question'], passages=[])\n",
        "\n",
        "    w_f1.append(metric_f1(w_pred, ans_true))\n",
        "    w_em.append(metric_exact_match(w_pred, ans_true))\n",
        "\n",
        "    wo_f1.append(metric_f1(wo_pred, ans_true))\n",
        "    wo_em.append(metric_exact_match(wo_pred, ans_true))\n",
        "\n",
        "\n",
        "# TODO: Print mean of the exact match and mean of F1 scores for the model with and without prepended passage\n",
        "print(f'\\nw passages:\\tF1:{np.mean(w_f1):.2f},\\tEM: {np.mean(w_em):.2f}')\n",
        "print(f'w/o passages:\\tF1:{np.mean(wo_f1):.2f},\\tEM: {np.mean(wo_em):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x562pF1X9gtg"
      },
      "source": [
        "You will likely see improvements in scores by providing a passage to the model.\n",
        "\n",
        "In contrast to the previous evaluation, during inference in a real world scenario, we do not have access to the ground truth passage.\n",
        "All we have access to is the question from a user.\n",
        "Luckily, the company is providing you with an unstructured knowledge base. This could be the whole of Wikipedia but in our scenario, we use all the passages from the SQuAD dataset and shuffle them to remove any existing structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL7N2sKD9gtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd8c2c2-6910-49a3-f799-c256e32a1952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2067 passages in the knowledge base\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "kb = list(set(dataset[\"validation\"][\"context\"]))\n",
        "\n",
        "# make sure that there is no remaining structure\n",
        "random.Random(42).shuffle(kb)\n",
        "print(len(kb), \"passages in the knowledge base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWQi9Z2C9gtg"
      },
      "source": [
        "Now whenever we receive a question, we need to find the relevant passage(s) from the knowledge base and put it in the model input.\n",
        "This is a non-trivial task and a whole research field of Information Retrieval is devoted to it.\n",
        "\n",
        "\n",
        "We are going to convert all the knowledge base passages into vectors using TF-IDF and the provided embedding model ([bert-base-nli-max-tokens](https://huggingface.co/sentence-transformers/bert-base-nli-max-tokens)).\n",
        "The model inference is already implemented for you but you need to fill in all the functions in the `KnowledgeBase` class.\n",
        "You will need to implement the retrieval, the distance metrics, and the three similarity metrics (Euclidean, cosine, inner product).\n",
        "\n",
        "We need to build an abstraction for the knowledge base. It needs to support:\n",
        "- adding new keys (vectors) and their corresponding values\n",
        "- retrieving the closest key given one, based on 3 vector distance metrics\n",
        "\n",
        "The implementation does not need to be efficient.\n",
        "\n",
        "Hint: it's ok to just add all the elements to a list and on retrieval sort the list by the distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFKeERqlCthx"
      },
      "outputs": [],
      "source": [
        "# Knowledge base building. This cell requires <20 new lines.\n",
        "from typing import Literal, List, Any\n",
        "\n",
        "Vec = List\n",
        "Val = Any\n",
        "\n",
        "class KnowledgeBase:\n",
        "    def __init__(self, dim: int):\n",
        "        \"\"\"\n",
        "        Initialize a knowledge base with a given dimensionality.\n",
        "        :param dim: the dimensionality of the vectors to be stored\n",
        "        \"\"\"\n",
        "        # TODO: initialize a persistent structure, such as a simple list\n",
        "        self.data = list()\n",
        "\n",
        "    def add_item(self, key: Vec, val: Val):\n",
        "        \"\"\"\n",
        "        Store the key-value pair in the knowledge base.\n",
        "        :param key: key\n",
        "        :param val: value\n",
        "        \"\"\"\n",
        "        # TODO: add to the persistent structure\n",
        "        self.data.append({'key': key, 'val': val})\n",
        "        self.dist_metrics = {\n",
        "            'l2': self._sim_euclidean,\n",
        "            'cos': self._sim_cosine,\n",
        "            'ip': self._sim_inner_product\n",
        "        }\n",
        "\n",
        "    def retrieve(\n",
        "        self, key: Vec, metric: Literal['l2', 'cos', 'ip'], k: int = 1\n",
        "    ) -> List[Val]:\n",
        "        \"\"\"\n",
        "        Retrieve the top k values from the knowledge base given a key and similarity metric.\n",
        "        :param key: key\n",
        "        :param metric: Similarity metric to use.\n",
        "        :param k: Top k similar items to retrieve.\n",
        "        :return: List of top k similar values.\n",
        "        \"\"\"\n",
        "        # TODO: retrieve the k closest vectors and return their corresponding values\n",
        "        # Hint: this does not have to be efficient, feel free to just sort the whole persistent structure and return the top k\n",
        "        dists = [self.dist_metrics[metric](key, x['key']) for x in self.data]\n",
        "        return [x['val'] for x in self.data[np.argsort(dists)[:k]]]\n",
        "\n",
        "    @staticmethod\n",
        "    def _sim_euclidean(a: Vec, b: Vec) -> float:\n",
        "        \"\"\"\n",
        "        Compute Euclidean (L2) distance between two vectors.\n",
        "        :param a: Vector a\n",
        "        :param b: Vector b\n",
        "        :return: Similarity score\n",
        "        \"\"\"\n",
        "        # hint: use numpy\n",
        "        # TODO: compute the Euclidean distance between two vectors\n",
        "        return np.linalg.norm(np.array(a) - np.array(b))\n",
        "\n",
        "    @staticmethod\n",
        "    def _sim_cosine(a: Vec, b: Vec) -> float:\n",
        "        \"\"\"\n",
        "        Compute the cosine similarity between two vectors.\n",
        "        :param a: Vector a\n",
        "        :param b: Vector b\n",
        "        :return: Similarity score\n",
        "        \"\"\"\n",
        "        # hint: use numpy\n",
        "        # TODO: compute the cosine distance between two vectors\n",
        "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "    @staticmethod\n",
        "    def _sim_inner_product(a: Vec, b: Vec) -> float:\n",
        "        \"\"\"\n",
        "        Compute the inner product between two vectors.\n",
        "        :param a: Vector a\n",
        "        :param b: Vector b\n",
        "        :return: Similarity score\n",
        "        \"\"\"\n",
        "        # hint: use numpy\n",
        "        # TODO: compute the iner product distance between two vectors\n",
        "        return np.dot(a, b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUv2PWPqCthx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7a6aa9-62a3-4472-90f3-e4d3c89d27ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "100%|██████████| 2067/2067 [00:27<00:00, 73.93it/s]\n"
          ]
        }
      ],
      "source": [
        "# Build knowledge base index\n",
        "# In ideal case this does not need to be changed and can just be run.\n",
        "# Make modifications if you feel they are necessary.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# # Sparse retrieval using TF-IDF - vectorize with tfidf and retrieve\n",
        "vectorizer = TfidfVectorizer(max_features=768, norm=None)\n",
        "kb_vectorized = np.asarray(vectorizer.fit_transform([x for x in kb]).todense())\n",
        "kb_index_tfidf = KnowledgeBase(dim=768)\n",
        "for passage_index, passage_embd in enumerate(kb_vectorized):\n",
        "    kb_index_tfidf.add_item(passage_embd.squeeze(), passage_index)\n",
        "\n",
        "# Dense retrieval using Sentence Transformers\n",
        "model_embd = SentenceTransformer(\"bert-base-nli-mean-tokens\").to(\"cuda:0\")\n",
        "kb_index_embed = KnowledgeBase(dim=768)\n",
        "for passage_index, passage_embd in enumerate(tqdm.tqdm(kb)):\n",
        "    kb_index_embed.add_item(model_embd.encode(passage_embd).squeeze(), passage_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlyVH1Zr9gtg"
      },
      "source": [
        "For the same first 500 questions from the validation split evaluate how often is the retrieved passage the correct one (formally Recall@1) or among the top 5 retrieved (Recall@5).\n",
        "Perform the retrieval with three distance metrics: euclidean distance, cosine distance, and inner product. The result for this should be 12 numbers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def evaluate_retrieval(metric, k):\n",
        "    embed_succ, tfidf_succ = 0, 0\n",
        "    for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "        # embed query\n",
        "        q_embed = model_embd.encode(line['question']).squeeze()\n",
        "        q_tfidf = vectorizer.transform(line['question']).todense()\n",
        "\n",
        "        # retrieve closest indices\n",
        "        embed_retrieved = kb_index_embed.retrieve(q_embed, metric=metric, k=k)\n",
        "        tfidf_retrieved = kb_index_tfidf.retrieve(q_tfidf, metric=metric, k=k)\n",
        "\n",
        "        # get gt indices\n",
        "        gt_index = kb.index(line['context'])\n",
        "        embed_succ += gt_index in embed_retrieved\n",
        "        tfidf_succ += gt_index in tfidf_retrieved\n",
        "\n",
        "    return embed_succ, tfidf_succ\n",
        "\n",
        "\n",
        "metrics = [\"l2\", \"cos\", \"ip\"]\n",
        "K = [1, 5]\n",
        "for metric, k in itertools.product(metrics, K):\n",
        "    print(f'k: {k}, metric: {metric.upper()}')\n",
        "    embed_succ, tfidf_succ = evaluate_retrieval(metric, k)\n",
        "    print(f'Embed: {embed_succ/500:.2f}\\tTF-IDF: {tfidf_succ/500:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "V7Df5QZLqy6H",
        "outputId": "80f57356-a943-45ea-f5a6-076032fa9ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k: 1\n",
            "metric: L2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/500 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "ufunc 'subtract' did not contain a loop with signature matching types (dtype('<U52'), dtype('float64')) -> None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-1c7d0510a9a0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0membed_succ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_succ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mtfidf_retrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkb_index_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0membed_retrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkb_index_embd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-a11d45460367>\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, key, metric, k)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# TODO: retrieve the k closest vectors and return their corresponding values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Hint: this does not have to be efficient, feel free to just sort the whole persistent structure and return the top k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-a11d45460367>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# TODO: retrieve the k closest vectors and return their corresponding values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Hint: this does not have to be efficient, feel free to just sort the whole persistent structure and return the top k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-a11d45460367>\u001b[0m in \u001b[0;36m_sim_euclidean\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# hint: use numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# TODO: compute the Euclidean distance between two vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'subtract' did not contain a loop with signature matching types (dtype('<U52'), dtype('float64')) -> None"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzfSY98f9gtg"
      },
      "source": [
        "In production, you receive a question from the user and to answer it, you need to first retrieve the relevant passage(s), pass it to the model, and only then generate the answer.\n",
        "\n",
        "Evaluate the model performance with passages retrieved by TFIDF and EMBD vectorization.\n",
        "Consider top-1 and top-5 passages.\n",
        "This time use only case-insensitive F1.\n",
        "The result for this cell should be |vectorizations $\\times$ passage sizes $\\times$ distance metrics = 2 x 2 x 3 = 12 numbers.\n",
        "\n",
        "Answer the following questions:\n",
        "* Based on the results, what are the advantages and disadvantages of using multiple retrieved passages?\n",
        "* Describe one approach to detect if none of the retrieved passages is relevant to the user question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oazFAFIBP2u2"
      },
      "source": [
        "TODO:\n",
        "* YOUR ANSWER HERE\n",
        "* YOUR ANSWER HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF9KJ8dXCthx"
      },
      "outputs": [],
      "source": [
        "# In ideal case this does not need to be changed and can just be run.\n",
        "# Make modifications if you feel they are necessary.\n",
        "\n",
        "for metric in [\"l2\", \"cos\", \"ip\"]:\n",
        "    print(metric.upper())\n",
        "\n",
        "    for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "        # TODO: evaluate the retrieval\n",
        "        # TODO: store RAG model output\n",
        "        # This requires <30 new lines\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rokgCOi9gtg"
      },
      "source": [
        "Answer the following questions about similarity metrics:\n",
        "* Compare and contrast the three metrics, what they might be influenced by, and their advantages and disadvantages.\n",
        "* Consider the scenario if the vectors in the knowledge base were normalized so that $|x|_2 = 1$. What would the results look like? Hint: look at the formulas with this vector assumption.\n",
        "* Answer what the Recall@k of the three distance metrics is relative to each other (i.e. which vector metric is the best and which is the worst one?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oFY9cn_Qhvr"
      },
      "source": [
        "TODO:\n",
        "\n",
        "* YOUR ANSWER HERE\n",
        "* YOUR ANSWER HERE\n",
        "* YOUR ANSWER HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBtKQELq9gtg"
      },
      "source": [
        "Lastly, it is a good practice to analyze failure cases of your solution to better understand the pipeline.\n",
        "Find the first example of each and compute how often the situation happens (percentage). Use the maximum exact match to determine correctness and L2 + embedding for retrieval.\n",
        "\n",
        "- For top-1: The retrieved passage is **correct** but the model is **not correct**.\n",
        "- For top-1: The retrieved passage is **not correct** but the model is **still correct**.\n",
        "- For top-5: One of the retrieved passages is the **correct** one but the model is **not correct**.\n",
        "- For top-1: Without retrieved passage is the model **correct** but with the passage the model becomes **incorrect**.\n",
        "- For top-1: Without retrieved passage is the model **incorrect** and with the passage the model becomes **incorrect** but in a different way (different answer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX0zSnot9gth"
      },
      "outputs": [],
      "source": [
        "# compute the 5 phenomena statistics (relative frequency) and find examples\n",
        "\n",
        "# TODO: <30 lines\n",
        "for line in tqdm.tqdm(dataset[\"validation\"].select(range(500))):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fo6hkfq9gti"
      },
      "source": [
        "A client is complaining that the model answers incorrectly the question _\"Who is the current Governor of Victoria?\"_.\n",
        "1. Show your model output to this question with top-1 retrieved passage using any metric.\n",
        "2. Show which top-1 context is retrieved by L2 embd.\n",
        "\n",
        "Hint for the correct answer, see: [en.wikipedia.org/wiki/Premier_of_Victoria](https://en.wikipedia.org/wiki/Premier_of_Victoria)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTa9yP289gti"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Who is the premier of Victoria?\"\n",
        "# TODO: < 20 lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBnDF4nTOe7"
      },
      "source": [
        "Answer the following questions:\n",
        "* Provide a reason why your model is giving the incorrect answer. (information tracing)\n",
        "* Propose a way by which this could be remedied. (information editing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddscx0DUTTiX"
      },
      "source": [
        "TODO:\n",
        "* YOUR ANSWER HERE\n",
        "* YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KcDRiRl9gti"
      },
      "source": [
        "Note on compute: the GPU time of the gold solution is ~15 minutes. If your solution requires much more compute (e.g. hours), then you are likely doing something incorrectly."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "046b96a8882979e38267a32a551607f1ba06d2ceaf740f4fcb11894ced88ffd3"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}